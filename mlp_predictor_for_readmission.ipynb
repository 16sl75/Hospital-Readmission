{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861000e5a373abe8",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:56:28.570367Z",
     "start_time": "2024-02-23T03:56:26.108047700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.metrics import *\n",
    "#from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning Begins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dab385d9af0bbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \"\"\"\n",
    "    Until where indicated, all code taken from\n",
    "\n",
    "    Long, A. (2020, January 30).\n",
    "        Using Machine Learning to Predict Hospital Readmission for Patients with\n",
    "        Diabetes with Scikit-Learn. Retrieved November 21, 2020,\n",
    "        from https://towardsdatascience.com/predicting-hospital-readmission-for-\n",
    "        patients-with-diabetes-using-scikit-learn-a2e359b15f0\n",
    "        \n",
    "    Except for some comments, which I've added\n",
    "    \"\"\"\n",
    "    \n",
    "Except for some comments, which I've added\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Until where indicated, all code taken from\n",
    "https://github.com/vignesh-bhat1999/predicting_hospital_readmissions\n",
    "Vignesh B Svignesh-bhat1999\n",
    "Machine learning and deep learning enthusiast\n",
    "All notes are preserved for better understanding of the Data Cleaning method"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9728920c35174af5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def dataPrep(fileName):\n",
    "\n",
    "    \"\"\"\n",
    "    Until where indicated, all code taken from\n",
    "\n",
    "    Long, A. (2020, January 30).\n",
    "        Using Machine Learning to Predict Hospital Readmission for Patients with\n",
    "        Diabetes with Scikit-Learn. Retrieved November 21, 2020,\n",
    "        from https://towardsdatascience.com/predicting-hospital-readmission-for-\n",
    "        patients-with-diabetes-using-scikit-learn-a2e359b15f0\n",
    "\n",
    "    Except for some comments, which I've added\n",
    "    \"\"\"\n",
    "\n",
    "    #DATA EXPLORATION\n",
    "    \n",
    "    # load the csv file\n",
    "    df = pd.read_csv(fileName)\n",
    "    #print('Number of samples:',len(df)) #flag\n",
    "    #gets the columns\n",
    "    #df.info()\n",
    "\n",
    "    # count the number of rows for each type\n",
    "    df.groupby('readmitted').size() #gives number of readmissions\n",
    "    df.groupby('discharge_disposition_id').size()\n",
    "    \n",
    "    #removing rows of patients who died or went to hospice\n",
    "    df = df.loc[~df.discharge_disposition_id.isin([11,13,14,19,20,21])]\n",
    "\n",
    "    #output variable to make binary classifications with\n",
    "    #will predict whether a patient will be readmitted within 30 days of their\n",
    "    #discharge from the hospital\n",
    "    df['OUTPUT_LABEL'] = (df.readmitted == '<30').astype('int')\n",
    "\n",
    "    #print('Prevalence:%.3f'%calc_prevalence(df['OUTPUT_LABEL'].values)) #flag\n",
    "\n",
    "    #FEATURE ENGINEERING\n",
    "    \n",
    "    df[list(df.columns)[:10]].head()\n",
    "    df[list(df.columns)[10:20]].head()\n",
    "    df[list(df.columns)[20:30]].head()\n",
    "    df[list(df.columns)[30:40]].head()\n",
    "    df[list(df.columns)[40:]].head()\n",
    "\n",
    "    # for each column\n",
    "    for c in list(df.columns):\n",
    "\n",
    "        # get a list of unique values\n",
    "        n = df[c].unique()\n",
    "\n",
    "        #flags\n",
    "        \"\"\"\n",
    "        # if number of unique values is less than 30, print the values. Otherwise print the number of unique values\n",
    "        if len(n)<30:\n",
    "            print(c)\n",
    "            print(n)\n",
    "        else:\n",
    "            print(c + ': ' +str(len(n)) + ' unique values')\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    # replace ? with nan\n",
    "    df = df.replace('?',np.nan)\n",
    "\n",
    "    #NUMERICAL FEATURES\n",
    "    cols_num = ['time_in_hospital','num_lab_procedures', 'num_procedures',\n",
    "                'num_medications', 'number_outpatient', 'number_emergency',\n",
    "                'number_inpatient','number_diagnoses']\n",
    "\n",
    "    df[cols_num].isnull().sum()\n",
    "\n",
    "    #CATEGORICAL FEATURES\n",
    "    cols_cat = ['race', 'gender',\n",
    "           'max_glu_serum', 'A1Cresult',\n",
    "           'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "           'glimepiride', 'acetohexamide', 'glipizide', 'glyburide',\n",
    "           'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "           'miglitol', 'troglitazone', 'tolazamide', 'insulin',\n",
    "           'glyburide-metformin', 'glipizide-metformin',\n",
    "           'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "           'metformin-pioglitazone', 'change', 'diabetesMed','payer_code']\n",
    "\n",
    "    df[cols_cat].isnull().sum()\n",
    "\n",
    "    #replacing Na/NaN values with \"UNK\"\n",
    "    df['race'] = df['race'].fillna('UNK')\n",
    "    df['payer_code'] = df['payer_code'].fillna('UNK')\n",
    "    df['medical_specialty'] = df['medical_specialty'].fillna('UNK')\n",
    "\n",
    "    #print('Number medical specialty:', df.medical_specialty.nunique()) #flag\n",
    "    df.groupby('medical_specialty').size().sort_values(ascending = False)\n",
    "\n",
    "    #Top 10 features from medical specialty\n",
    "    top_10 = ['UNK','InternalMedicine','Emergency/Trauma',\\\n",
    "              'Family/GeneralPractice', 'Cardiology','Surgery-General' ,\\\n",
    "              'Nephrology','Orthopedics',\\\n",
    "              'Orthopedics-Reconstructive','Radiologist']\n",
    "\n",
    "    # make a new column with duplicated data\n",
    "    df['med_spec'] = df['medical_specialty'].copy()\n",
    "\n",
    "    # replace all specialties not in top 10 with 'Other' category\n",
    "    df.loc[~df.med_spec.isin(top_10),'med_spec'] = 'Other'\n",
    "\n",
    "    df.groupby('med_spec').size()\n",
    "    cols_cat_num = ['admission_type_id', 'discharge_disposition_id',\n",
    "                    'admission_source_id']\n",
    "    df[cols_cat_num] = df[cols_cat_num].astype('str')\n",
    "    df_cat = pd.get_dummies(df[cols_cat + cols_cat_num + ['med_spec']],\n",
    "                            drop_first = True)\n",
    "    df_cat.head()\n",
    "    df = pd.concat([df,df_cat], axis = 1)\n",
    "    cols_all_cat = list(df_cat.columns)\n",
    "\n",
    "    #EXTRA FEATURES\n",
    "    df[['age', 'weight']].head()\n",
    "    df.groupby('age').size()\n",
    "    #reformatting age_id from non-numeric to numeric\n",
    "    age_id = {'[0-10)':0,\n",
    "              '[10-20)':10,\n",
    "              '[20-30)':20,\n",
    "              '[30-40)':30,\n",
    "              '[40-50)':40,\n",
    "              '[50-60)':50,\n",
    "              '[60-70)':60,\n",
    "              '[70-80)':70,\n",
    "              '[80-90)':80,\n",
    "              '[90-100)':90}\n",
    "    df['age_group'] = df.age.replace(age_id)\n",
    "    df.weight.notnull().sum()\n",
    "    df['has_weight'] = df.weight.notnull().astype('int')\n",
    "    cols_extra = ['age_group','has_weight']\n",
    "\n",
    "    #flags\n",
    "    \"\"\"\n",
    "    print('Total number of features:', len(cols_num + cols_all_cat + cols_extra))\n",
    "    print('Numerical Features:',len(cols_num))\n",
    "    print('Categorical Features:',len(cols_all_cat))\n",
    "    print('Extra features:',len(cols_extra))\n",
    "    \"\"\"\n",
    "\n",
    "    df[cols_num + cols_all_cat + cols_extra].isnull().sum().sort_values(ascending = False).head(10)\n",
    "    col2use = cols_num + cols_all_cat + cols_extra\n",
    "    df_data = df[col2use + ['OUTPUT_LABEL']]\n",
    "\n",
    "    # shuffle the samples\n",
    "    df_data = df_data.sample(n = len(df_data), random_state = 42)\n",
    "    df_data = df_data.reset_index(drop = True)\n",
    "\n",
    "    # # Save 30% of the data as validation and test data\n",
    "    # df_valid_test=df_data.sample(frac=0.30,random_state=42)\n",
    "    # #print('Split size: %.3f'%(len(df_valid_test)/len(df_data))) #flag\n",
    "    # \n",
    "    # df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\n",
    "    # df_valid = df_valid_test.drop(df_test.index)\n",
    "    # \n",
    "    # # use the rest of the data as training data\n",
    "    # df_train_all=df_data.drop(df_valid_test.index)\n",
    "    \n",
    "    #flags\n",
    "    \"\"\"\n",
    "    print('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.OUTPUT_LABEL.values)))\n",
    "    print('Valid prevalence(n = %d):%.3f'%(len(df_valid),calc_prevalence(df_valid.OUTPUT_LABEL.values)))\n",
    "    print('Train all prevalence(n = %d):%.3f'%(len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values)))\n",
    "    \"\"\"\n",
    "    \n",
    "    #print('all samples (n = %d)'%len(df_data)) #flag\n",
    "    assert len(df_data) == (len(df_data)),'math didnt work'\n",
    "\n",
    "    # split the training data into positive and negative\n",
    "    rows_pos = df_data.OUTPUT_LABEL == 1\n",
    "    df_train_pos = df_data.loc[rows_pos]\n",
    "    df_train_neg = df_data.loc[~rows_pos]\n",
    "\n",
    "    # merge the balanced data\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n",
    "\n",
    "    # shuffle the order of training samples\n",
    "    df_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "    \"\"\"\n",
    "    print('Train balanced prevalence(n = %d):%.3f'%(len(df_train), \\\n",
    "                    calc_prevalence(df_train.OUTPUT_LABEL.values))) #flag\n",
    "    \"\"\"\n",
    "    \n",
    "    # df_train_all.to_csv('df_train_all.csv',index=False)\n",
    "    df_train.to_csv('df_train.csv',index=False)\n",
    "    # df_valid.to_csv('df_valid.csv',index=False)\n",
    "    # df_test.to_csv('df_test.csv',index=False)\n",
    "\n",
    "    X_ = df_train[col2use].values\n",
    "    # X_train_all = df_train_all[col2use].values\n",
    "    # X_valid = df_valid[col2use].values\n",
    "\n",
    "    y_ = df_train['OUTPUT_LABEL'].values\n",
    "    # y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "\n",
    "    #flags\n",
    "    \"\"\"\n",
    "    print('Training All shapes:',X_train_all.shape)\n",
    "    print('Training shapes:',X_train.shape, y_train.shape)\n",
    "    print('Validation shapes:',X_valid.shape, y_valid.shape)\n",
    "    \"\"\"\n",
    "\n",
    "    scaler  = StandardScaler()\n",
    "    scaler.fit(X_)\n",
    "    scalerfile = 'scaler.sav'\n",
    "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "    # load it back\n",
    "    scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "\n",
    "    X_ = scaler.transform(X_)\n",
    "    # X_valid_tf = scaler.transform(X_valid)\n",
    "\n",
    "    return X_, y_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:56:30.368105600Z",
     "start_time": "2024-02-23T03:56:30.339393600Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "training_file = 'C2T1_Train.csv'\n",
    "testing_file = 'C2T1_Test.csv'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:56:31.074668300Z",
     "start_time": "2024-02-23T03:56:31.050028300Z"
    }
   },
   "id": "c2df0fc2be6e41cb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X_train, y_train = dataPrep(training_file)\n",
    "X_test, y_test = dataPrep(training_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:56:36.813435Z",
     "start_time": "2024-02-23T03:56:31.678479300Z"
    }
   },
   "id": "d1d83e231d3a4d2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation Done!!!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ec1664e8efe604d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLP Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4190be0940e1025"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learningRate=0.01, nIterations=1000):\n",
    "        self.learningRate = learningRate\n",
    "        self.nIterations = nIterations\n",
    "        self.activationFunc = self._stepFunc\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    #goes through the training data x and attempts to fit it to y\n",
    "    def fit(self, x,y):\n",
    "        # num of rows is the number of samples\n",
    "        # num of cols is the number of features\n",
    "        nSamples,nFeatures = x.shape\n",
    "\n",
    "        #setting weights to 0 for every feature\n",
    "        self.weights = np.zeros(nFeatures)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.nIterations):\n",
    "            #enumerate function gives index and current sample\n",
    "            for idx, x_i in enumerate(x):\n",
    "\n",
    "                #gets prediction\n",
    "                yPredicted = self.predict(x_i)\n",
    "\n",
    "                #checks which way we need to adjust the weights\n",
    "                if (y[idx] > yPredicted):\n",
    "                    update = self.learningRate\n",
    "                elif(y[idx] < yPredicted):\n",
    "                    update = -self.learningRate\n",
    "                #good so dont change\n",
    "                else:\n",
    "                    update = 0\n",
    "\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    #used to predict current input with current weights + bias\n",
    "    def predict(self, x):\n",
    "        #takes the dot product\n",
    "        #w transpose * X + bias\n",
    "        linearOuput = np.dot(x, self.weights) + self.bias\n",
    "        #apply activation function\n",
    "        y_predicted = self.activationFunc(linearOuput)\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "    #activation function\n",
    "    def _stepFunc(self, x):\n",
    "        #allows x to be a single sample or x to be an array\n",
    "        #threshold\n",
    "        return np.where(x>=0, 1, 0)\n",
    "\n",
    "#perceptron + the addition of pocketalgo\n",
    "class PocketAlgo:\n",
    "    def __init__(self, learningRate=0.01, nIterations=1000):\n",
    "        self.learningRate = learningRate\n",
    "        self.nIterations = nIterations\n",
    "        self.activationFunc = self._stepFunc\n",
    "        self.longestRun = 0\n",
    "        self.weights = None\n",
    "        self.pocket = None\n",
    "        self.bias = None\n",
    "\n",
    "    #goes through the training data x and attempts to fit it to y\n",
    "    def fit(self, x,y):\n",
    "        # num of rows is the number of samples\n",
    "        # num of cols is the number of features\n",
    "        nSamples,nFeatures = x.shape\n",
    "\n",
    "        #initialize weights\n",
    "        #setting weights to 0 for every feature\n",
    "        self.weights = np.zeros(nFeatures)\n",
    "        self.pocket =  np.zeros(nFeatures)\n",
    "        self.bias = 0\n",
    "\n",
    "\n",
    "        for _ in range(self.nIterations):\n",
    "\n",
    "            currentLongestRun = 0\n",
    "            bestCurrentWeights = self.pocket.copy()\n",
    "            self.weights = self.pocket.copy()\n",
    "            #enumerate function gives index and current sample\n",
    "            for idx, x_i in enumerate(x):\n",
    "                currentLongestRun += 1\n",
    "                yPredicted = self.predict(x_i)\n",
    "\n",
    "                #checks if the current prediction is incorrect\n",
    "                #if so checks if a new weight vector needs to be replaced in the pocket\n",
    "\n",
    "                if(y[idx] != yPredicted):\n",
    "                    if(y[idx] > yPredicted):\n",
    "                        update = self.learningRate\n",
    "                    elif(y[idx] < yPredicted):\n",
    "                        update = -self.learningRate\n",
    "                    if (currentLongestRun > self.longestRun):\n",
    "                        bestCurrentWeights = self.weights.copy()\n",
    "                        self.longestRun = currentLongestRun\n",
    "                    currentLongestRun = 0\n",
    "\n",
    "                    self.weights += update * x_i\n",
    "                    self.bias += update\n",
    "\n",
    "\n",
    "            #sets the bestCurrentWeights to pocket- may not change the value of the pocket\n",
    "            self.pocket = bestCurrentWeights.copy()\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        #takes the dot product\n",
    "        #w transpose * X + bias\n",
    "        linearOuput = np.dot(x, self.weights) + self.bias\n",
    "        #apply activation function\n",
    "        y_predicted = self.activationFunc(linearOuput)\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "    #activation function\n",
    "    def _stepFunc(self, x):\n",
    "        #allows x to be a single sample or x to be an array\n",
    "        #threshold\n",
    "        return np.where(x>=0, 1, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:56:39.268638400Z",
     "start_time": "2024-02-23T03:56:39.220086800Z"
    }
   },
   "id": "e76c605732f4127c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training of the MMLP Begins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f311660ea4e3850"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "classifyReadmissionsPerception = PocketAlgo(learningRate=0.001, nIterations=1000)\n",
    "classifyReadmissionsPerception.fit(X_train, y_train) # fitting\n",
    "\n",
    "y_train_preds = classifyReadmissionsPerception.predict(X_train)\n",
    "y_test_preds = classifyReadmissionsPerception.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T03:59:36.720177800Z",
     "start_time": "2024-02-23T03:56:40.428173600Z"
    }
   },
   "id": "c2466098c55b61bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specificity Report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2a2b6b7ea7e996"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "thresh =0.5\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))\n",
    "\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    print(y_pred)\n",
    "    print((y_pred >thresh))\n",
    "    print(y_actual)\n",
    "    #y_actual list of [1,0,0,0,1..]\n",
    "    #y_pred list of decimals 0-1\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T04:02:49.779581400Z",
     "start_time": "2024-02-23T04:02:49.741214700Z"
    }
   },
   "id": "5ccbcf3bceb9a4be"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"\"):\n",
    "    print(\"--------------------\"+str_title+\"Confusion Matrix Finishied----------------------------------\")\n",
    "    print(\"The accuracy is:\", accuracy)\n",
    "#     print(\"PRECISION for NOT_Readmiteed_in_30days:\",TN/(TN+FN))\n",
    "#     print(\"RECALL        for NOT_Readmiteed_in_30days:\",TN/(TN+FP))\n",
    "#     print(\"SPECIFICITY   for NOT_Readmiteed_in_30days:\",TP/(TN+FP))    \n",
    "#         print(\"Accuracy  for NOT_Readmiteed_in_30days:\",(TP + TN)/(TP+TN+FP+FN))\n",
    "    print(\"PRECISION for   Readmiteed_in_30days:\",precision)\n",
    "    print(\"RECALL    for   Readmiteed_in_30days:\",recall)\n",
    "    print(\"SPECIFICITY for Readmiteed_in_30days:\",specificity)\n",
    "#         print(\"Accuracy for      Readmiteed_in_30days:\",(TP + TN)/(TP+TN+FP+FN))\n",
    "    print(\"ROC_AUC=\",auc)\n",
    "    print(\"Prevalence=\",calc_prevalence(y_train))\n",
    "    print(50*\"--\")\n",
    "\n",
    "    return "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T04:02:50.451248Z",
     "start_time": "2024-02-23T04:02:50.426818200Z"
    }
   },
   "id": "26e7f6eaff3ab880"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perception\n",
      "On training data\n",
      "[0.50275566 0.53983799 0.53714795 ... 0.64114694 0.68115906 0.60485204]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[1 1 1 ... 1 1 0]\n",
      "AUC:0.667\n",
      "accuracy:0.615\n",
      "recall:0.548\n",
      "precision:0.633\n",
      "specificity:0.682\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.6149328859060402\n",
      "PRECISION for   Readmiteed_in_30days: 0.6327368061096547\n",
      "RECALL    for   Readmiteed_in_30days: 0.5478681405448085\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.681997631267272\n",
      "ROC_AUC= 0.6666220360253731\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Test:\n",
      "[0.50275566 0.53983799 0.53714795 ... 0.64114694 0.68115906 0.60485204]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[1 1 1 ... 1 1 0]\n",
      "AUC:0.667\n",
      "accuracy:0.615\n",
      "recall:0.548\n",
      "precision:0.633\n",
      "specificity:0.682\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.6149328859060402\n",
      "PRECISION for   Readmiteed_in_30days: 0.6327368061096547\n",
      "RECALL    for   Readmiteed_in_30days: 0.5478681405448085\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.681997631267272\n",
      "ROC_AUC= 0.6666220360253731\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Perception\")\n",
    "print('On training data')\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_train,y_train_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"\")\n",
    "print('\\n\\n\\nTest:')\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_test,y_test_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_test,str_title=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T04:03:21.535100600Z",
     "start_time": "2024-02-23T04:03:21.460605200Z"
    }
   },
   "id": "2814ce9b5d557745"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c04e55e565d14c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50275566 0.53983799 0.53714795 ... 0.64114694 0.68115906 0.60485204]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[1 1 1 ... 1 1 0]\n",
      "AUC:0.667\n",
      "accuracy:0.615\n",
      "recall:0.548\n",
      "precision:0.633\n",
      "specificity:0.682\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.6149328859060402\n",
      "PRECISION for   Readmiteed_in_30days: 0.6327368061096547\n",
      "RECALL    for   Readmiteed_in_30days: 0.5478681405448085\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.681997631267272\n",
      "ROC_AUC= 0.6666220360253731\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Test:\n",
      "[0.50275566 0.53983799 0.53714795 ... 0.64114694 0.68115906 0.60485204]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[1 1 1 ... 1 1 0]\n",
      "AUC:0.667\n",
      "accuracy:0.615\n",
      "recall:0.548\n",
      "precision:0.633\n",
      "specificity:0.682\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.6149328859060402\n",
      "PRECISION for   Readmiteed_in_30days: 0.6327368061096547\n",
      "RECALL    for   Readmiteed_in_30days: 0.5478681405448085\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.681997631267272\n",
      "ROC_AUC= 0.6666220360253731\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ann = MLPClassifier(learning_rate='constant',learning_rate_init=0.001, max_iter= 10000, activation='logistic',solver='sgd',hidden_layer_sizes=(35),random_state=1)\n",
    "ann.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_train_preds = ann.predict_proba(X_train)[:,1]\n",
    "y_test_preds = ann.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_train,y_train_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"\")\n",
    "print('\\n\\n\\nTest:')\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_test,y_test_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_test,str_title=\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T04:04:15.823902700Z",
     "start_time": "2024-02-23T04:04:05.454889Z"
    }
   },
   "id": "660700bbfe470489"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "461ca262a94ad02a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
