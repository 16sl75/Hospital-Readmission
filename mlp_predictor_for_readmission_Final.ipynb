{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861000e5a373abe8",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T17:03:24.739889Z",
     "start_time": "2024-02-22T17:03:21.607406800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.metrics import *\n",
    "#from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imblearn) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cathy\\.conda\\envs\\cisc839_a2\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T17:03:29.231352400Z",
     "start_time": "2024-02-22T17:03:25.525370900Z"
    }
   },
   "id": "46ef23ac267f82fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning Begins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dab385d9af0bbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \"\"\"\n",
    "    Until where indicated, all code taken from\n",
    "\n",
    "    Long, A. (2020, January 30).\n",
    "        Using Machine Learning to Predict Hospital Readmission for Patients with\n",
    "        Diabetes with Scikit-Learn. Retrieved November 21, 2020,\n",
    "        from https://towardsdatascience.com/predicting-hospital-readmission-for-\n",
    "        patients-with-diabetes-using-scikit-learn-a2e359b15f0\n",
    "        \n",
    "    Except for some comments, which I've added\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9728920c35174af5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def categorize_diagnosis(data,new,old):\n",
    "    '''\n",
    "    The diagnosis columns are converted from icd9 codes to one of the 9 categories.\n",
    "    The function creates a new column which consists of one of the 10 categories of disease.\n",
    "    Nan values are considered as a new category\n",
    "    '''\n",
    "    #copying old column into new column\n",
    "    data[new] = data[old]\n",
    "    #filling NaN values with -1 category\n",
    "    data[new] = data[new].fillna(-1)\n",
    "    #if the code contians V or E then it is considered as category 0.\n",
    "    data.loc[data[new].str.contains('V',na=False), [new]] = 0\n",
    "    data.loc[data[new].str.contains('E',na=False), [new]] = 0\n",
    "    #converting string column to float.\n",
    "    data[new] = data[new].astype(float)\n",
    "    #iterating through all the rows of the dataframe\n",
    "    for index, row in data.iterrows():\n",
    "        #checking if the code of the row belong to category 1,\n",
    "        if (row[new] >= 390 and row[new] < 460) or (np.floor(row[new]) == 785):\n",
    "            #assigning category 1 to the new column of the row.\n",
    "            data.loc[index, new] = 1\n",
    "        #checking if the code of the row belong to category 2,\n",
    "        elif (row[new] >= 460 and row[new] < 520) or (np.floor(row[new]) == 786):\n",
    "            #assigning category 2 to the new column of the row.\n",
    "            data.loc[index, new] = 2\n",
    "        elif (row[new] >= 520 and row[new] < 580) or (np.floor(row['new_diag1']) == 787):\n",
    "            data.loc[index, new] = 3\n",
    "        elif (np.floor(row[new]) == 250):\n",
    "            data.loc[index, new] = 4\n",
    "        elif (row[new] >= 800 and row[new] < 1000):\n",
    "            data.loc[index, new] = 5\n",
    "        elif (row[new] >= 710 and row[new] < 740):\n",
    "            data.loc[index, new] = 6\n",
    "        elif (row[new] >= 580 and row[new] < 630) or (np.floor(row[new]) == 788):\n",
    "            data.loc[index, new] = 7\n",
    "        elif (row[new] >= 140 and row[new] < 240):\n",
    "            data.loc[index, new] = 8\n",
    "        elif row[new] > 0 :\n",
    "            #if the code of the row does not belong to any category then it is given 0.\n",
    "            data.loc[index, new] = 0\n",
    "\n",
    "def data_prep(fileName):\n",
    "    \"\"\"\n",
    "    Until where indicated, all code taken from\n",
    "    https://github.com/vignesh-bhat1999/predicting_hospital_readmissions\n",
    "    Vignesh B Svignesh-bhat1999\n",
    "    Machine learning and deep learning enthusiast\n",
    "    All notes are preserved for better understanding of the Data Cleaning method\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"line359 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    data = pd.read_csv(fileName)\n",
    "    data.head()\n",
    "    \n",
    "    '''Data preprocessing'''\n",
    "    \n",
    "    #data contains multiple inpatient visits for some patients, I considered only the first encounter for each patient as mentioned in the research paper to determine whether or not they were readmitted within 30days. So the duplicate values are removed.\n",
    "    #creating new column duplicate which contains whether the row is duplicated or not. It is boolean.\n",
    "    if fileName == 'C2T1_Train.csv':\n",
    "        data['duplicate'] = data['patient_nbr2'].duplicated()\n",
    "    else:\n",
    "        data['duplicate'] = data['patient_nbr'].duplicated()\n",
    "    #only those rows which are not duplicated are kept in the dataset.\n",
    "    data = data[data['duplicate'] == False]\n",
    "    #the duplicate column is dropped.\n",
    "    data = data.drop(['duplicate'], axis = 1)\n",
    "    #removing the patients who are dead or in hospice.\n",
    "    data = data.loc[~data['discharge_disposition_id'].isin([11,13,14,19,20,21])]\n",
    "    #resetting index of the dataframe\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(['index'],axis=1)\n",
    "    #replacing '?' with NaN\n",
    "    data = data.replace('?',np.nan)\n",
    "    #let us check how many percentage of data in each column is nan\n",
    "    # print((data.isna().sum()/len(data))*100)\n",
    "    #We can observe that weight has the heighest nan values at 96% of data with nan values. Medical speciality and payer code have 48% and 43% nan values respectively.\n",
    "    #The race and diagnosis columns also have nan values but are less in number.\n",
    "    #The weight column can be dropped since there is very high percentage of nan values. The payer code and medical specialty column missing values can be found using imputation techniques since more than 50% data is available in both cases.\n",
    "    #removing weight,payer_code and medical_specialty columns\n",
    "    data = data.drop(['weight'], axis = 1)\n",
    "    \n",
    "    '''Exploratory data analysis'''\n",
    "    '''Univariate Analysis'''\n",
    "    \n",
    "    # Race\n",
    "    # the race column consists of Caucasian,AfricanAmerican, Hispanic, Asian and other as categories. It consists of 2.7% NaN values\n",
    "    #checking value counts\n",
    "    # print(data['race'].value_counts())\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    #plotting count plot with seaborn\n",
    "    # sns.countplot(data['race']).set_title('Count of race');\n",
    "    # plt.show()\n",
    "    #filling nan values of race with mode\n",
    "    data['race'].fillna(data['race'].mode()[0], inplace=True)\n",
    "    \n",
    "    # Gender\n",
    "    # The gender column tells us whether the patient is male or female.\n",
    "    # print(data['gender'].value_counts())\n",
    "    #3 values are unknown. We can either fill these values or drop the rows. Dropping the rows would be better since the data is also considered as invalid/Unknown incase of gender.\n",
    "    #dropping rows with gender as invalid.\n",
    "    data = data[data.gender != 'Unknown/Invalid']\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # sns.countplot(data['gender']).set_title('Count of gender');\n",
    "    # plt.show()\n",
    "    # The female count is more than male count but the difference is small.\n",
    "    # I have changed the label of male to 1 and female to 0.\n",
    "    data['gender'] = data['gender'].replace('Male', 1)\n",
    "    data['gender'] = data['gender'].replace('Female', 0)\n",
    "    \n",
    "    # Age\n",
    "    # print(data['age'].value_counts())\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # sns.countplot(data['age']).set_title('Count of age');\n",
    "    # plt.show()\n",
    "    # As expected, the patients with age less than 40 years are less in number when compared to patients with age greater than 40 years.\n",
    "    # The number of patients are highest in the age group of 70-80 years.\n",
    "    # I will be grouping the age feature into 3 categories as mentioned in the research paper.\n",
    "    #custom encoding age\n",
    "    data.loc[data['age'] == '[0-10)', ['age']] = 0\n",
    "    data.loc[data['age'] == '[10-20)', ['age']] = 0\n",
    "    data.loc[data['age'] == '[20-30)', ['age']] = 0\n",
    "    data.loc[data['age'] == '[30-40)', ['age']] = 1\n",
    "    data.loc[data['age'] == '[40-50)', ['age']] = 1\n",
    "    data.loc[data['age'] == '[50-60)', ['age']] = 1\n",
    "    data.loc[data['age'] == '[60-70)', ['age']] = 2\n",
    "    data.loc[data['age'] == '[70-80)', ['age']] = 2\n",
    "    data.loc[data['age'] == '[80-90)', ['age']] = 2\n",
    "    data.loc[data['age'] == '[90-100)', ['age']] = 2\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # sns.countplot(data['age']).set_title('Count of age');\n",
    "    # plt.show()\n",
    "    \n",
    "    # Admission_type_id\n",
    "    # Obtained from mappings given in uiuc:\n",
    "    # print(data['admission_type_id'].value_counts())\n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # sns.countplot(data['admission_type_id']).set_title('Count of admission type');\n",
    "    # In Admission_type_id most of the patients are admitted with id emergency, followed by Elective. *Some of the patients admission type id is not available. Null and Not mapped categories are also present.\n",
    "    \n",
    "    ### Discharge_disposition_id\n",
    "    # print(data['discharge_disposition_id'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['discharge_disposition_id']).set_title('Count of discharge disposition id');\n",
    "    #grouping patients who are transfered to another hospital\n",
    "    data.loc[(data['discharge_disposition_id'] == 3) | (data['discharge_disposition_id'] == 4)|\n",
    "             (data['discharge_disposition_id'] == 5) | (data['discharge_disposition_id'] == 16)|\n",
    "             (data['discharge_disposition_id'] == 22) |(data['discharge_disposition_id'] == 23)|\n",
    "             (data['discharge_disposition_id'] == 24)|(data['discharge_disposition_id'] == 10),\n",
    "             ['discharge_disposition_id']] = 2 \n",
    "    #grouping patients who are dischareed to home under some conditions\n",
    "    data.loc[(data['discharge_disposition_id'] == 6) | (data['discharge_disposition_id'] == 8)\n",
    "             ,['discharge_disposition_id']] = 1\n",
    "    #grouping patients who are transfered in the same hospital\n",
    "    data.loc[(data['discharge_disposition_id'] == 15) | (data['discharge_disposition_id'] == 17)|\n",
    "             (data['discharge_disposition_id'] == 9),['discharge_disposition_id']] = 3\n",
    "    #grouping patients who are invalid, null or unknown\n",
    "    data.loc[(data['discharge_disposition_id'] == 25) | (data['discharge_disposition_id'] == 26)|\n",
    "             (data['discharge_disposition_id'] == 18),['discharge_disposition_id']] = 4\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['discharge_disposition_id']).set_title('Count of discharge disposition id');\n",
    "    # * The discharge_disposition_id column is divided into 21 different categories which was then changed to 8 categories after careful observations\n",
    "    # * We can observe that most of the patients are discharged to home.\n",
    "    # * The patients who have passed away or in hospice are not present since we have already removed those rows from the data.\n",
    "    \n",
    "    \n",
    "    ### admission_source_id\n",
    "    # print(data['admission_source_id'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['admission_source_id']).set_title('Count of admission source id');\n",
    "    #grouping patients who are referred\n",
    "    data.loc[(data['admission_source_id'] == 2) | (data['admission_source_id'] == 3),\n",
    "             ['admission_source_id']] = 1 \n",
    "    #grouping patients who are transfered.\n",
    "    data.loc[(data['admission_source_id'] == 5) | (data['admission_source_id'] == 6)|\n",
    "             (data['admission_source_id'] == 10) | (data['admission_source_id'] == 22)|\n",
    "             (data['admission_source_id'] == 25),['admission_source_id']] = 4 \n",
    "    #grouping patients who are null/invalid or not mapped.\n",
    "    data.loc[(data['admission_source_id'] == 15) | (data['admission_source_id'] == 17)|\n",
    "             (data['admission_source_id'] == 20) | (data['admission_source_id'] == 21),\n",
    "             ['admission_source_id']] = 9\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['admission_source_id']).set_title('Count of admission source id');\n",
    "    # admission_source_id mappings are given in the ids_mappings.csv present in UCI.\n",
    "    # The categories were changed from 17 to 8.\n",
    "    # We can observe that most of the patients admission source is emergency room, followed by referals.\n",
    "    \n",
    "    \n",
    "    # Time_in_hospital\n",
    "    # print(data['time_in_hospital'].value_counts())\n",
    "    # print(data['time_in_hospital'].describe())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['time_in_hospital']).set_title('Count of time in hospital');\n",
    "    # The time in hospital column categorizes the patients stay ranging from 1 day to 14 days.\n",
    "    # The patients on average stay 4 days and most patients stay 3-4 days.\n",
    "    # The patients rarely stay more than 12 days.\n",
    "    # We can observe a positive skew in the plot.\n",
    "    \n",
    "    ### num_lab_procedures\n",
    "    # Refers to number of lab tests performed during the encounter\n",
    "    # print(data['num_lab_procedures'].describe())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.distplot(data['num_lab_procedures']).set_title('number of lab procedures');\n",
    "    # We can observe that on average 43 lab procedures are done during a patient encounter.\n",
    "    # A spike is also found near 0-2 procedures which suggest less number of lab tests were done on some patients.\n",
    "    \n",
    "    \n",
    "    ### Num_procedures\n",
    "    # Refers to number of procedures (other than lab tests) performed during the encounter\n",
    "    # print(data['num_procedures'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['num_procedures']).set_title('number of other procedures');\n",
    "    # It is a categorical feature.\n",
    "    # Most of the patients do not perform tests other than lab tests.\n",
    "    \n",
    "    ### Num_medications\n",
    "    # Refers to number of distinct generic names administered during the encounter\n",
    "    # print(data['num_medications'].describe())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.distplot(data['num_medications']).set_title('number of medications');\n",
    "    # data[data['num_medications']>70]\n",
    "    # Most of the patients are provided 16 medications on average.\n",
    "    # Only 7 patients are given more than 70 medications.\n",
    "    # The plot has positive skewness and resembles normal distribution to a cetain extent\n",
    "    \n",
    "    \n",
    "    # Number_outpatient\n",
    "    # Refers to number of outpatient visits of the patient in the year preceding the encounter\n",
    "    # print(data['number_outpatient'].describe())\n",
    "    # print(data['number_outpatient'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.distplot(data['number_outpatient']).set_title('number of outpatient visits');\n",
    "    # We can observe that most of the patients do not have any outpatient visits.\n",
    "    # Very less patients have more than 15 outpatient visits\n",
    "    \n",
    "    \n",
    "    # Number_emergency\n",
    "    # Refers to number of emergency visits of the patient in the year preceding the encounter\n",
    "    # print(data['number_emergency'].describe())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.distplot(data['number_emergency']).set_title('number of emergency visits');\n",
    "    # It is similar to number_outpatient distplot.\n",
    "    # We can observe that most of the patients do not have any emergency visits.\n",
    "    \n",
    "    # Number_inpatient\n",
    "    # Refers to number of inpatient visits of the patient in the year preceding the encounter\n",
    "    # print(data['number_inpatient'].describe())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['number_inpatient']).set_title('number of inpatient visits');\n",
    "    # We can observe that most of the patients do not have any inpatient visits.\n",
    "    # It is similar to other visit figures seen.\n",
    "    # We can create a new feature 'visits' which will be some of inpatient, outpatient and emergency visits since all three are distributed in similar ways.\n",
    "    \n",
    "    \n",
    "    # Diagnosis\n",
    "    # All three columns contain code which are categorized into one of the 9 groups given below.\n",
    "    # We can categorize these codes into the 9 categories given above and use them as diagnosis of diseases which come under these 9 categories. This idea has been taken from the research paper.\n",
    "    #https://www.kaggle.com/iabhishekofficial/prediction-on-hospital-readmission.\n",
    "    # def categorize_diagnosis(data,new,old):\n",
    "    #     '''\n",
    "    #     The diagnosis columns are converted from icd9 codes to one of the 9 categories.\n",
    "    #     The function creates a new column which consists of one of the 10 categories of disease.\n",
    "    #     Nan values are considered as a new category\n",
    "    #     '''\n",
    "    #     #copying old column into new column\n",
    "    #     data[new] = data[old]\n",
    "    #     #filling NaN values with -1 category\n",
    "    #     data[new] = data[new].fillna(-1)\n",
    "    #     #if the code contians V or E then it is considered as category 0.\n",
    "    #     data.loc[data[new].str.contains('V',na=False), [new]] = 0\n",
    "    #     data.loc[data[new].str.contains('E',na=False), [new]] = 0\n",
    "    #     #converting string column to float.\n",
    "    #     data[new] = data[new].astype(float)\n",
    "    #     #iterating through all the rows of the dataframe\n",
    "    #     for index, row in data.iterrows():\n",
    "    #         #checking if the code of the row belong to category 1,\n",
    "    #         if (row[new] >= 390 and row[new] < 460) or (np.floor(row[new]) == 785):\n",
    "    #             #assigning category 1 to the new column of the row.\n",
    "    #             data.loc[index, new] = 1\n",
    "    #         #checking if the code of the row belong to category 2,\n",
    "    #         elif (row[new] >= 460 and row[new] < 520) or (np.floor(row[new]) == 786):\n",
    "    #             #assigning category 2 to the new column of the row.\n",
    "    #             data.loc[index, new] = 2\n",
    "    #         elif (row[new] >= 520 and row[new] < 580) or (np.floor(row['new_diag1']) == 787):\n",
    "    #             data.loc[index, new] = 3\n",
    "    #         elif (np.floor(row[new]) == 250):\n",
    "    #             data.loc[index, new] = 4\n",
    "    #         elif (row[new] >= 800 and row[new] < 1000):\n",
    "    #             data.loc[index, new] = 5\n",
    "    #         elif (row[new] >= 710 and row[new] < 740):\n",
    "    #             data.loc[index, new] = 6\n",
    "    #         elif (row[new] >= 580 and row[new] < 630) or (np.floor(row[new]) == 788):\n",
    "    #             data.loc[index, new] = 7\n",
    "    #         elif (row[new] >= 140 and row[new] < 240):\n",
    "    #             data.loc[index, new] = 8\n",
    "    #         elif row[new] > 0 :\n",
    "    #             #if the code of the row does not belong to any category then it is given 0.\n",
    "    #             data.loc[index, new] = 0\n",
    "    print(\"line603 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    categorize_diagnosis(data,'new_diag1','diag_1')\n",
    "    categorize_diagnosis(data,'new_diag2','diag_2')\n",
    "    categorize_diagnosis(data,'new_diag3','diag_3')\n",
    "    print(\"line607 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    # newly created diagnois\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['new_diag1']).set_title('number of patients diagnosed');\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['new_diag2']).set_title('number of patients diagnosed');\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['new_diag3']).set_title('number of patients diagnosed');\n",
    "    # plt.show()\n",
    "    data.drop(['diag_1','diag_2','diag_3'],axis=1,inplace=True)\n",
    "    # In the second and additional second diagnosis we can observe that more number of patients are getting diagnosed with 4 which is diabetes mellitus.\n",
    "    # Most of the patients are diagnosed with respiratory and other disease types.\n",
    "    # The nan category also increase with diagnosis number.\n",
    "    \n",
    "    # Number_diagnoses\n",
    "    # Refers to the number of diagnoses entered to the system\n",
    "    # print(data['number_diagnoses'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['number_diagnoses']).set_title('number of diagnosis');\n",
    "    # Most patients have undergone 9 diagnoses.\n",
    "    # More than 9 diagnoses is rare.\n",
    "    \n",
    "    \n",
    "    # Max_glu_serum\n",
    "    # Indicates the range of the result or if the test was not taken. Values: \">200,\"\">300,\"\"normal,\" and \"none\" if not measured\n",
    "    # print(data['max_glu_serum'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['max_glu_serum']).set_title('range of glucose serum test result');\n",
    "    # Ordinal encoding is done since max_glu_serum above certain values indicate the value is abnormal for the patient and hence are more important\n",
    "    #ordinal encoding max_glu_serum\n",
    "    data.loc[data['max_glu_serum'] == 'None', ['max_glu_serum']] = 0\n",
    "    data.loc[data['max_glu_serum'] == 'Norm', ['max_glu_serum']] = 1\n",
    "    data.loc[data['max_glu_serum'] == '>200', ['max_glu_serum']] = 2\n",
    "    data.loc[data['max_glu_serum'] == '>300', ['max_glu_serum']] = 3\n",
    "    # Most of the patients dont undergo this test.\n",
    "    # Out of the people who undergo this test about half of the patients result are normal, the other half patients result are either in category >200 or >300.\n",
    "    \n",
    "    \n",
    "    # A1Cresult\n",
    "    # Indicates the range of the result or if the test was not taken. Values: \">8\" if the result was greater than 8%, \">7\" if the result was greater than 7% but less than 8%, \"normal\" if the result was less than 7%, and \"none\" if not measured.\n",
    "    # print(data['A1Cresult'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['A1Cresult']).set_title('range of A1Cresult');\n",
    "    # plt.show()\n",
    "    # Ordinal encoding is done since A1Cresult above certain values indicate the value is abnormal for the patient and hence are more important\n",
    "    #Ordinal encoding A1Cresult\n",
    "    data.loc[data['A1Cresult'] == 'None', ['A1Cresult']] = 0\n",
    "    data.loc[data['A1Cresult'] == 'Norm', ['A1Cresult']] = 1\n",
    "    data.loc[data['A1Cresult'] == '>7', ['A1Cresult']] = 2\n",
    "    data.loc[data['A1Cresult'] == '>8', ['A1Cresult']] = 3\n",
    "    # Most of the patients dont undergo this test.\n",
    "    # Out of the people who undergo this test nearly half of the patients result are >8, the other half patients result are either in >7 or normal category.\n",
    "    \n",
    "    \n",
    "    # Medications\n",
    "    # Values: \"up\" if the dosagewas increased during the encounter, \"down\" if the dosage was decreased, \"steady\" if thedosage did not change, and \"no\" if the drug was not prescribed\n",
    "    cols = ['metformin', 'repaglinide', 'nateglinide',\n",
    "           'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n",
    "           'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "           'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n",
    "           'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "           'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "           'metformin-pioglitazone']\n",
    "    medications = data[cols]\n",
    "    # for i in cols:\n",
    "    #     plt.figure(figsize=(15,5))\n",
    "    #     print(medications[i].value_counts())\n",
    "    #     sns.countplot(medications[i]).set_title(i);\n",
    "    # plt.show()\n",
    "    # From the above plots we can observe that examide, citoglipton and glimepiride-pioglitazone have all the feature values as No. These 3 features dont help to classify whether the patient readmitted within 30 days as all the values are same. So lets drop these features from the dataset.\n",
    "    data.drop(['examide', 'citoglipton','glimepiride-pioglitazone'],axis=1,inplace=True)\n",
    "    #ordinal encoding medication columns\n",
    "    medics = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "           'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "           'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "           'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "           'metformin-rosiglitazone', 'metformin-pioglitazone']\n",
    "    for i in medics:\n",
    "        data.loc[data[i] == 'Up', [i]] = 2  \n",
    "        data.loc[data[i] == 'Down', [i]] = 2 \n",
    "        data.loc[data[i] == 'Steady', [i]] = 1 \n",
    "        data.loc[data[i] == 'No', [i]] = 0 \n",
    "    # The medications can be merged into a single feature and the number of medications a patients has taken can be calculated.\n",
    "    # The custom encoding of medication was done with any change in dosage resutling in 2, steady given as 1 and if the medication was not required then it is represented as 0.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Change\n",
    "    # Indicates if there was a change in diabetic medications (either dosage or genericname). Values: \"change\" and \"no change\"\n",
    "    # print(data['change'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['change']).set_title('change in medicine');\n",
    "    # plt.show()\n",
    "    data.loc[data['change'] == 'No', ['change']] = 0\n",
    "    data.loc[data['change'] == 'Ch', ['change']] = 1\n",
    "    # More than 50% of patients did not get any changes in the medicine, the other patients changed medicines.\n",
    "    # The change feature was encoded with 0 representing no change and 1 representing change in medication.\n",
    "    \n",
    "    \n",
    "    # DiabetesMed\n",
    "    # Indicates if there was any diabetic medication prescribed. Values: \"yes\" and \"no\"\n",
    "    # print(data['diabetesMed'].value_counts())\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['diabetesMed']).set_title('diabetic medication prescribed');\n",
    "    # plt.show()\n",
    "    data.loc[data['diabetesMed'] == 'No', ['diabetesMed']] = 0\n",
    "    data.loc[data['diabetesMed'] == 'Yes', ['diabetesMed']] = 1\n",
    "    # Most of the patients were prescribed diabetes medication.\n",
    "    # The diabetesMed feature was encoded with 0 representing not prescribed and 1 representing medicine prescribed.\n",
    "    \n",
    "    \n",
    "    # Readmitted\n",
    "    # This is the variable which we must predict.\n",
    "    # It refers to days to inpatient readmission. Values: \"<30\" if the patient was readmitted in less than 30 days, \">30\" if the patient was readmitted in more than 30 days, and \"No\" for no record of readmission\n",
    "#     print(data['readmitted'].value_counts())\n",
    "    data.loc[data['readmitted'] == '<30', ['readmitted']] = 1\n",
    "    data.loc[data['readmitted'] == '>30', ['readmitted']] = 0 \n",
    "    data.loc[data['readmitted'] == 'NO', ['readmitted']] = 0\n",
    "    # plt.figure(figsize=(15,5))\n",
    "    # sns.countplot(data['readmitted']).set_title('Readmitted');\n",
    "#     print(data['readmitted'].value_counts()/len(data))\n",
    "    # We must predict whether the patient is readmitted within 30 days.\n",
    "    # From the graph we can observe that less number of people are readmitted within 30 days and most of the people are either not readmitted or are readmitted after 30 days.\n",
    "    # oversampling/ undersampling techniques will be required to make the data balanced.\n",
    "    \n",
    "    \n",
    "    data.to_csv('before_imputation.csv',index=False)\n",
    "    # Integer identifier corresponding to 23 distinct values, for example, Blue Cross\\BlueShield, Medicare, and self-pay\n",
    "    \n",
    "    \n",
    "    # Payer_code before imputation\n",
    "    data = pd.read_csv('D:/case_study_1/before_imputation.csv')\n",
    "    data['payer_code'].value_counts()\n",
    "    data['payer_code'].isna().sum()\n",
    "    # We have 30414 nan values which is roughly 43.4% of the data in the payer_code column.\n",
    "    \n",
    "    # plt.figure(figsize=(10,5))\n",
    "    # sns.countplot(data['payer_code']).set_title('payer code');\n",
    "    #fig=plt.figure() \n",
    "    # line1 = plt.plot(data['payer_code'].value_counts(),'ko-',label='line1')\n",
    "    # Most of the patient payment was done by medicare(MC).\n",
    "    # The other payments are not used much, only MC, HM and BC are used.\n",
    "    # plt.show()\n",
    "    \n",
    "    # medical_specialty before imputation\n",
    "    # Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family\\general practice, and surgeon\n",
    "    # print(data['medical_specialty'].value_counts())\n",
    "    # plt.figure(figsize=(20,50))\n",
    "    # sns.countplot(y = data['medical_specialty']).set_title('medical specialty');\n",
    "    # plt.show()\n",
    "    # Most of the patients were admitted under InternalMedicine physician.\n",
    "    # Family/GeneralPractice, Cardiology and Emergency/Trauma physicians also accounted for some patients.\n",
    "    # Other categories patients are very less in number.\n",
    "    \n",
    "    \n",
    "    # Predicting nan values in payer_code and medical_speciality through imputation.\n",
    "    #making a deep copy of dataframe.\n",
    "    data1 = data.copy()\n",
    "    #checking columns\n",
    "#     print(\"data1.columns=\\n\",data1.columns)\n",
    "    # Custom encoding is done on the copy of data, so data is not affected.\n",
    "    # custom encoding 17 categories of payer code.\n",
    "    data1.loc[data1['payer_code'] == 'MD', ['payer_code']] = 1\n",
    "    data1.loc[data1['payer_code'] == 'MC', ['payer_code']] = 2\n",
    "    data1.loc[data1['payer_code'] == 'HM', ['payer_code']] = 3\n",
    "    data1.loc[data1['payer_code'] == 'UN', ['payer_code']] = 4\n",
    "    data1.loc[data1['payer_code'] == 'BC', ['payer_code']] = 5\n",
    "    data1.loc[data1['payer_code'] == 'CP', ['payer_code']] = 6\n",
    "    data1.loc[data1['payer_code'] == 'SP', ['payer_code']] = 7\n",
    "    data1.loc[data1['payer_code'] == 'SI', ['payer_code']] = 8\n",
    "    data1.loc[data1['payer_code'] == 'CM', ['payer_code']] = 9\n",
    "    data1.loc[data1['payer_code'] == 'DM', ['payer_code']] = 10\n",
    "    data1.loc[data1['payer_code'] == 'CH', ['payer_code']] = 11\n",
    "    data1.loc[data1['payer_code'] == 'PO', ['payer_code']] = 12\n",
    "    data1.loc[data1['payer_code'] == 'WC', ['payer_code']] = 13\n",
    "    data1.loc[data1['payer_code'] == 'OG', ['payer_code']] = 14\n",
    "    data1.loc[data1['payer_code'] == 'OT', ['payer_code']] = 15\n",
    "    data1.loc[data1['payer_code'] == 'MP', ['payer_code']] = 16\n",
    "    data1.loc[data1['payer_code'] == 'FR', ['payer_code']] = 17\n",
    "    # custom encoding 17 categories of payer code. Here custom encoding is done on data.\n",
    "    data.loc[data['payer_code'] == 'MD', ['payer_code']] = 1\n",
    "    data.loc[data['payer_code'] == 'MC', ['payer_code']] = 2\n",
    "    data.loc[data['payer_code'] == 'HM', ['payer_code']] = 3\n",
    "    data.loc[data['payer_code'] == 'UN', ['payer_code']] = 4\n",
    "    data.loc[data['payer_code'] == 'BC', ['payer_code']] = 5\n",
    "    data.loc[data['payer_code'] == 'CP', ['payer_code']] = 6\n",
    "    data.loc[data['payer_code'] == 'SP', ['payer_code']] = 7\n",
    "    data.loc[data['payer_code'] == 'SI', ['payer_code']] = 8\n",
    "    data.loc[data['payer_code'] == 'CM', ['payer_code']] = 9\n",
    "    data.loc[data['payer_code'] == 'DM', ['payer_code']] = 10\n",
    "    data.loc[data['payer_code'] == 'CH', ['payer_code']] = 11\n",
    "    data.loc[data['payer_code'] == 'PO', ['payer_code']] = 12\n",
    "    data.loc[data['payer_code'] == 'WC', ['payer_code']] = 13\n",
    "    data.loc[data['payer_code'] == 'OG', ['payer_code']] = 14\n",
    "    data.loc[data['payer_code'] == 'OT', ['payer_code']] = 15\n",
    "    data.loc[data['payer_code'] == 'MP', ['payer_code']] = 16\n",
    "    data.loc[data['payer_code'] == 'FR', ['payer_code']] = 17\n",
    "    #using label encoder to convert different categories into numeric labels.\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(['Pediatrics-Endocrinology', 'InternalMedicine',\n",
    "           'Family/GeneralPractice', 'Cardiology', 'Surgery-General',\n",
    "           'Orthopedics', 'Gastroenterology',\n",
    "           'Surgery-Cardiovascular/Thoracic', 'Nephrology',\n",
    "           'Orthopedics-Reconstructive', 'Psychiatry', 'Emergency/Trauma',\n",
    "           'Pulmonology', 'Surgery-Neuro',\n",
    "           'Obsterics&Gynecology-GynecologicOnco', 'ObstetricsandGynecology',\n",
    "           'Pediatrics', 'Hematology/Oncology', 'Otolaryngology',\n",
    "           'Surgery-Colon&Rectal', 'Pediatrics-CriticalCare', 'Endocrinology',\n",
    "           'Urology', 'Psychiatry-Child/Adolescent', 'Pediatrics-Pulmonology',\n",
    "           'Anesthesiology-Pediatric', 'Radiology',\n",
    "           'Pediatrics-Hematology-Oncology', 'Psychology', 'Neurology',\n",
    "           'Podiatry', 'Gynecology', 'Oncology', 'Pediatrics-Neurology',\n",
    "           'Surgery-Thoracic', 'Surgery-PlasticwithinHeadandNeck',\n",
    "           'Surgery-Plastic', 'Ophthalmology', 'Surgery-Pediatric',\n",
    "           'Pediatrics-EmergencyMedicine',\n",
    "           'PhysicalMedicineandRehabilitation', 'InfectiousDiseases',\n",
    "           'AllergyandImmunology', 'Surgery-Maxillofacial', 'Dentistry',\n",
    "           'Surgeon', 'Surgery-Vascular', 'Osteopath', 'Psychiatry-Addictive',\n",
    "           'Surgery-Cardiovascular', 'Anesthesiology', 'PhysicianNotFound',\n",
    "           'Hematology', 'Proctology', 'Rheumatology', 'Obstetrics',\n",
    "           'SurgicalSpecialty', 'Radiologist', 'Dermatology', 'Pathology',\n",
    "           'SportsMedicine', 'Speech', 'Hospitalist', 'OutreachServices',\n",
    "           'Cardiology-Pediatric', 'Perinatology', 'Neurophysiology',\n",
    "           'Endocrinology-Metabolism', 'DCPTEAM', 'Resident'])\n",
    "    a = le.transform(data1[data1['medical_specialty'].notnull()]['medical_specialty'])\n",
    "    # replacing the encoded non nan data1.\n",
    "    data1.loc[data1['medical_specialty'].notnull(), ['medical_specialty']] = a\n",
    "    # replacing the encoded non nan data.\n",
    "    data.loc[data['medical_specialty'].notnull(), ['medical_specialty']] = a\n",
    "    \n",
    "    \n",
    "    # Only predicting for medical speciality.\n",
    "    #separating the columns that contain null values and need to predicted.\n",
    "    nancols = ['medical_specialty']\n",
    "    notnancols = ['admission_type_id',     'discharge_disposition_id',     'admission_source_id',\n",
    "                  'time_in_hospital',     'num_lab_procedures',     'num_procedures',     'num_medications',     \n",
    "                  'number_diagnoses','number_outpatient', 'number_emergency', 'number_inpatient',     'new_diag1',     'new_diag2',     'new_diag3','metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "           'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "           'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "           'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "           'metformin-rosiglitazone', 'metformin-pioglitazone','A1Cresult','max_glu_serum']\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # selecting not nan data from columns consisting nan values. This data will be used from training and testing.\n",
    "    notnans = data1[nancols].notnull().all(axis=1)\n",
    "    df_notnans = data1[notnans]\n",
    "    \n",
    "    data.to_csv('df_notnans.csv',index=False)\n",
    "    #########################################################################################################################\n",
    "    # feature engineering\n",
    "    # #############################_\n",
    "    \n",
    "    data1.to_csv('D:/case_study_1/data1.csv',index=False)\n",
    "    \n",
    "    # visits feature\n",
    "    # As seen in univariate analysis the inpatient visits, outpatient visits and emergency visits can be combined into a single feature called visits.\n",
    "    \n",
    "    data = pd.read_csv('D:/case_study_1/data1.csv')\n",
    "    data['visits'] = data['number_inpatient'] + data['number_outpatient'] +data['number_emergency']\n",
    "    # Since many of the patients didnt get visited by anyone, we can make the visit feature binary meaning the feature can be whether the patient got visits or not. This can be done by replacing the patients who got visits by 1.\n",
    "    \n",
    "    data.loc[data['visits']>0,'visits']=1\n",
    "    data.drop(['number_inpatient','number_outpatient','number_emergency'],axis=1,inplace=True)\n",
    "    # the inpatient, outpatient and emergency columns can be dropped.\n",
    "    \n",
    "    \n",
    "    ### number of steady medicines feature and increase/decrease of medicine given to each patient feature\n",
    "    \n",
    "    # list of all medication column\n",
    "    medics = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "           'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "           'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "           'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "           'metformin-rosiglitazone', 'metformin-pioglitazone']\n",
    "    \n",
    "    #new dataframe for medication\n",
    "    medications = data.loc[:,medics]\n",
    "    \n",
    "    #initialize empty list.\n",
    "    steady=[]\n",
    "    up_or_down = []\n",
    "    #iterate through each row in medication dataframe and find number of 1's and 2's present in each row.\n",
    "    print(\"line889 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    \n",
    "    for i in range(len(medications)):\n",
    "        try:#check if 1's are present.\n",
    "            if(medications.loc[i].value_counts()[1]>0):#if present then append to steady.\n",
    "                steady.append(medications.loc[i].value_counts()[1])\n",
    "    #             print(\"i=\",i)\n",
    "        except KeyError:#if not present then keyerror message is displayed, then append 0.\n",
    "            steady.append(0)\n",
    "        try:#check if 2's are present.\n",
    "            if(medications.loc[i].value_counts()[2]>0):#if present then append to up_or_down.\n",
    "                up_or_down.append(medications.loc[i].value_counts()[2])\n",
    "    #             print(\"i=\",i)\n",
    "        except KeyError:#if not present then keyerror message is displayed, then append 0.\n",
    "            up_or_down.append(0)    \n",
    "    print(\"line904 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "           \n",
    "    #converting to numpy array\n",
    "    up_or_down = np.array(up_or_down)\n",
    "    steady = np.array(steady)\n",
    "    \n",
    "    #assigning to data dataframe as features.\n",
    "    data['up_or_down'] = up_or_down\n",
    "    data['steady'] = steady\n",
    "#     print(data.columns)\n",
    "    \n",
    "    ### Feature Selection\n",
    "    \n",
    "    #storing the dataframe for future use.\n",
    "    data.to_csv('data_true',index=False)\n",
    "    #reading data\n",
    "    data = pd.read_csv('data_true')\n",
    "    #one hot encoding of categorical features.\n",
    "    df_pd = pd.get_dummies(data, columns=['race', 'gender', 'admission_type_id', 'discharge_disposition_id',\n",
    "                                          'admission_source_id', 'new_diag1','new_diag2','new_diag3','medical_specialty','payer_code'], drop_first = True)\n",
    "    \n",
    "    #keeping only those features which will help in predicting\n",
    "    X = df_pd.drop(['readmitted','encounter_id','patient_nbr'],axis=1)\n",
    "    #feature to be predicted\n",
    "    y=df_pd['readmitted']\n",
    "    \n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # #splitting into train and split\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0,stratify=y)\n",
    "    \n",
    "    #applying smote to oversample since the variable to be predicted is imbalanced and using smote only on train data.\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #splitting smote applied data.\n",
    "    # train_X, val_X, train_y, val_y = train_test_split(X_res, y_res, random_state=1)\n",
    "    print(\"line945 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    my_model = RandomForestClassifier(n_estimators=100,\n",
    "                                      random_state=0).fit(X, y)\n",
    "    print(\"line948 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "    #using permutation importance to choose the best features which contribute most in predicting the target variable.\n",
    "    import eli5\n",
    "    from eli5.sklearn import PermutationImportance\n",
    "    \n",
    "    perm = PermutationImportance(my_model, random_state=1).fit(X, y)\n",
    "    print(\"line954 time=\",time.asctime( time.localtime(time.time()) ))\n",
    "#     eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n",
    "#     a = eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n",
    "    \n",
    "    #only features having positive permutation importance are chosen and kept in a list.\n",
    "    feat = []\n",
    "    value = []\n",
    "    for i in range(len(perm.feature_importances_)):\n",
    "        if(perm.feature_importances_[i]>0):\n",
    "            feat.append(list(X_res.columns)[i])\n",
    "            value.append(perm.feature_importances_[i])\n",
    "    \n",
    "    #creating a new dataframe using the selected features.\n",
    "    feat_impo = X_res.loc[:,feat]\n",
    "    \n",
    "    #storing data in X and target variable in y\n",
    "    X = feat_impo.copy()\n",
    "    y = y_res\n",
    "    \"\"\"The following scalar is not included in the literature, I used the same method from Data Cleaning method 1\"\"\"\n",
    "    scaler  = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    scalerfile = 'scaler.sav'\n",
    "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "    # load it back\n",
    "    scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "    \"\"\"End of feature engineering\"\"\"\n",
    "\n",
    "    # X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    X_transformed = scaler.transform(X)\n",
    "    # X_valid_tf = scaler.transform(X_cv)\n",
    "\n",
    "    return X_transformed, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T17:03:36.123062100Z",
     "start_time": "2024-02-22T17:03:36.096534500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "training_file = 'C2T1_Train.csv'\n",
    "testing_file = 'C2T1_Test.csv'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T17:03:39.145250600Z",
     "start_time": "2024-02-22T17:03:39.128450700Z"
    }
   },
   "id": "c2df0fc2be6e41cb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line359 time= Fri Feb 23 01:03:58 2024\n",
      "line603 time= Fri Feb 23 01:04:00 2024\n",
      "line607 time= Fri Feb 23 01:05:22 2024\n",
      "line889 time= Fri Feb 23 01:05:26 2024\n",
      "line904 time= Fri Feb 23 01:06:49 2024\n",
      "line945 time= Fri Feb 23 01:06:53 2024\n",
      "line948 time= Fri Feb 23 01:07:11 2024\n",
      "line954 time= Fri Feb 23 01:34:14 2024\n",
      "line359 time= Fri Feb 23 01:34:14 2024\n",
      "line603 time= Fri Feb 23 01:34:15 2024\n",
      "line607 time= Fri Feb 23 01:35:43 2024\n",
      "line889 time= Fri Feb 23 01:35:47 2024\n",
      "line904 time= Fri Feb 23 01:37:27 2024\n",
      "line945 time= Fri Feb 23 01:37:31 2024\n",
      "line948 time= Fri Feb 23 01:37:50 2024\n",
      "line954 time= Fri Feb 23 02:04:27 2024\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = data_prep(training_file)\n",
    "X_test, y_test = data_prep(training_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T18:04:27.947052100Z",
     "start_time": "2024-02-22T17:03:58.650035100Z"
    }
   },
   "id": "d1d83e231d3a4d2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preparation Done!!!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ec1664e8efe604d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MLP Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4190be0940e1025"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learningRate=0.01, nIterations=1000):\n",
    "        self.learningRate = learningRate\n",
    "        self.nIterations = nIterations\n",
    "        self.activationFunc = self._stepFunc\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    #goes through the training data x and attempts to fit it to y\n",
    "    def fit(self, x,y):\n",
    "        # num of rows is the number of samples\n",
    "        # num of cols is the number of features\n",
    "        nSamples,nFeatures = x.shape\n",
    "\n",
    "        #setting weights to 0 for every feature\n",
    "        self.weights = np.zeros(nFeatures)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.nIterations):\n",
    "            #enumerate function gives index and current sample\n",
    "            for idx, x_i in enumerate(x):\n",
    "\n",
    "                #gets prediction\n",
    "                yPredicted = self.predict(x_i)\n",
    "\n",
    "                #checks which way we need to adjust the weights\n",
    "                if (y[idx] > yPredicted):\n",
    "                    update = self.learningRate\n",
    "                elif(y[idx] < yPredicted):\n",
    "                    update = -self.learningRate\n",
    "                #good so dont change\n",
    "                else:\n",
    "                    update = 0\n",
    "\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    #used to predict current input with current weights + bias\n",
    "    def predict(self, x):\n",
    "        #takes the dot product\n",
    "        #w transpose * X + bias\n",
    "        linearOuput = np.dot(x, self.weights) + self.bias\n",
    "        #apply activation function\n",
    "        y_predicted = self.activationFunc(linearOuput)\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "    #activation function\n",
    "    def _stepFunc(self, x):\n",
    "        #allows x to be a single sample or x to be an array\n",
    "        #threshold\n",
    "        return np.where(x>=0, 1, 0)\n",
    "\n",
    "#perceptron + the addition of pocketalgo\n",
    "class PocketAlgo:\n",
    "    def __init__(self, learningRate=0.01, nIterations=1000):\n",
    "        self.learningRate = learningRate\n",
    "        self.nIterations = nIterations\n",
    "        self.activationFunc = self._stepFunc\n",
    "        self.longestRun = 0\n",
    "        self.weights = None\n",
    "        self.pocket = None\n",
    "        self.bias = None\n",
    "\n",
    "    #goes through the training data x and attempts to fit it to y\n",
    "    def fit(self, x,y):\n",
    "        # num of rows is the number of samples\n",
    "        # num of cols is the number of features\n",
    "        nSamples,nFeatures = x.shape\n",
    "\n",
    "        #initialize weights\n",
    "        #setting weights to 0 for every feature\n",
    "        self.weights = np.zeros(nFeatures)\n",
    "        self.pocket =  np.zeros(nFeatures)\n",
    "        self.bias = 0\n",
    "\n",
    "\n",
    "        for _ in range(self.nIterations):\n",
    "\n",
    "            currentLongestRun = 0\n",
    "            bestCurrentWeights = self.pocket.copy()\n",
    "            self.weights = self.pocket.copy()\n",
    "            #enumerate function gives index and current sample\n",
    "            for idx, x_i in enumerate(x):\n",
    "                currentLongestRun += 1\n",
    "                yPredicted = self.predict(x_i)\n",
    "\n",
    "                #checks if the current prediction is incorrect\n",
    "                #if so checks if a new weight vector needs to be replaced in the pocket\n",
    "\n",
    "                if(y[idx] != yPredicted):\n",
    "                    if(y[idx] > yPredicted):\n",
    "                        update = self.learningRate\n",
    "                    elif(y[idx] < yPredicted):\n",
    "                        update = -self.learningRate\n",
    "                    if (currentLongestRun > self.longestRun):\n",
    "                        bestCurrentWeights = self.weights.copy()\n",
    "                        self.longestRun = currentLongestRun\n",
    "                    currentLongestRun = 0\n",
    "\n",
    "                    self.weights += update * x_i\n",
    "                    self.bias += update\n",
    "\n",
    "\n",
    "            #sets the bestCurrentWeights to pocket- may not change the value of the pocket\n",
    "            self.pocket = bestCurrentWeights.copy()\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        #takes the dot product\n",
    "        #w transpose * X + bias\n",
    "        linearOuput = np.dot(x, self.weights) + self.bias\n",
    "        #apply activation function\n",
    "        y_predicted = self.activationFunc(linearOuput)\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "    #activation function\n",
    "    def _stepFunc(self, x):\n",
    "        #allows x to be a single sample or x to be an array\n",
    "        #threshold\n",
    "        return np.where(x>=0, 1, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T18:05:13.064809800Z",
     "start_time": "2024-02-22T18:05:13.008770700Z"
    }
   },
   "id": "e76c605732f4127c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training of the MMLP Begins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f311660ea4e3850"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "classifyReadmissionsPerception = PocketAlgo(learningRate=0.001, nIterations=1000)\n",
    "classifyReadmissionsPerception.fit(X_train, y_train) # fitting\n",
    "\n",
    "y_train_preds = classifyReadmissionsPerception.predict(X_train)\n",
    "y_test_preds = classifyReadmissionsPerception.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T18:41:05.135164700Z",
     "start_time": "2024-02-22T18:05:14.925040900Z"
    }
   },
   "id": "c2466098c55b61bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specificity Report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2a2b6b7ea7e996"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "thresh =0.5\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))\n",
    "\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    print(y_pred)\n",
    "    print((y_pred >thresh))\n",
    "    print(y_actual)\n",
    "    #y_actual list of [1,0,0,0,1..]\n",
    "    #y_pred list of decimals 0-1\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:20:07.262147300Z",
     "start_time": "2024-02-22T19:20:07.239785700Z"
    }
   },
   "id": "b9bb6833ab2e6924"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"\"):\n",
    "    print(\"--------------------\"+str_title+\"Confusion Matrix Finishied----------------------------------\")\n",
    "    print(\"The accuracy is:\", accuracy)\n",
    "#     print(\"PRECISION for NOT_Readmiteed_in_30days:\",TN/(TN+FN))\n",
    "#     print(\"RECALL        for NOT_Readmiteed_in_30days:\",TN/(TN+FP))\n",
    "#     print(\"SPECIFICITY   for NOT_Readmiteed_in_30days:\",TP/(TN+FP))    \n",
    "#         print(\"Accuracy  for NOT_Readmiteed_in_30days:\",(TP + TN)/(TP+TN+FP+FN))\n",
    "    print(\"PRECISION for   Readmiteed_in_30days:\",precision)\n",
    "    print(\"RECALL    for   Readmiteed_in_30days:\",recall)\n",
    "    print(\"SPECIFICITY for Readmiteed_in_30days:\",specificity)\n",
    "#         print(\"Accuracy for      Readmiteed_in_30days:\",(TP + TN)/(TP+TN+FP+FN))\n",
    "    print(\"ROC_AUC=\",auc)\n",
    "    print(\"Prevalence=\",calc_prevalence(y_train))\n",
    "    print(50*\"--\")\n",
    "\n",
    "    return "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:40:10.963264200Z",
     "start_time": "2024-02-22T19:40:10.943215700Z"
    }
   },
   "id": "5ccbcf3bceb9a4be"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perception\n",
      "On training data\n",
      "[0.00737226 0.26675718 0.17663639 ... 0.99998552 0.99998809 0.99995084]\n",
      "[False False False ...  True  True  True]\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "127381    1\n",
      "127382    1\n",
      "127383    1\n",
      "127384    1\n",
      "127385    1\n",
      "Name: readmitted, Length: 127386, dtype: int64\n",
      "AUC:0.960\n",
      "accuracy:0.936\n",
      "recall:0.883\n",
      "precision:0.988\n",
      "specificity:0.989\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------TRAINING Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.9361625296343398\n",
      "PRECISION for   Readmiteed_in_30days: 0.9878650580404966\n",
      "RECALL    for   Readmiteed_in_30days: 0.8831739751621057\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.9891510841065737\n",
      "ROC_AUC= 0.9603559945236014\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Test:\n",
      "[0.00737226 0.26675718 0.17663639 ... 0.99998552 0.99998809 0.99995084]\n",
      "[False False False ...  True  True  True]\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "127381    1\n",
      "127382    1\n",
      "127383    1\n",
      "127384    1\n",
      "127385    1\n",
      "Name: readmitted, Length: 127386, dtype: int64\n",
      "AUC:0.960\n",
      "accuracy:0.936\n",
      "recall:0.883\n",
      "precision:0.988\n",
      "specificity:0.989\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------TESTING Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.9361625296343398\n",
      "PRECISION for   Readmiteed_in_30days: 0.9878650580404966\n",
      "RECALL    for   Readmiteed_in_30days: 0.8831739751621057\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.9891510841065737\n",
      "ROC_AUC= 0.9603559945236014\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Perception\")\n",
    "print('On training data')\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_train,y_train_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"TRAINING \")\n",
    "print('\\n\\n\\nTest:')\n",
    "auc, accuracy, recall, precision, specificity =print_report(y_test,y_test_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_test,str_title=\"TESTING \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:42:07.104932500Z",
     "start_time": "2024-02-22T19:42:06.003067300Z"
    }
   },
   "id": "2814ce9b5d557745"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison with Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c04e55e565d14c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "[0.00737226 0.26675718 0.17663639 ... 0.99998552 0.99998809 0.99995084]\n",
      "[False False False ...  True  True  True]\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "127381    1\n",
      "127382    1\n",
      "127383    1\n",
      "127384    1\n",
      "127385    1\n",
      "Name: readmitted, Length: 127386, dtype: int64\n",
      "AUC:0.960\n",
      "accuracy:0.936\n",
      "recall:0.883\n",
      "precision:0.988\n",
      "specificity:0.989\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------TRAINING Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.9361625296343398\n",
      "PRECISION for   Readmiteed_in_30days: 0.9878650580404966\n",
      "RECALL    for   Readmiteed_in_30days: 0.8831739751621057\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.9891510841065737\n",
      "ROC_AUC= 0.9603559945236014\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Test:\n",
      "[0.00737226 0.26675718 0.17663639 ... 0.99998552 0.99998809 0.99995084]\n",
      "[False False False ...  True  True  True]\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "127381    1\n",
      "127382    1\n",
      "127383    1\n",
      "127384    1\n",
      "127385    1\n",
      "Name: readmitted, Length: 127386, dtype: int64\n",
      "AUC:0.960\n",
      "accuracy:0.936\n",
      "recall:0.883\n",
      "precision:0.988\n",
      "specificity:0.989\n",
      "prevalence:0.500\n",
      " \n",
      "--------------------TESTING Confusion Matrix Finishied----------------------------------\n",
      "The accuracy is: 0.9361625296343398\n",
      "PRECISION for   Readmiteed_in_30days: 0.9878650580404966\n",
      "RECALL    for   Readmiteed_in_30days: 0.8831739751621057\n",
      "SPECIFICITY for Readmiteed_in_30days: 0.9891510841065737\n",
      "ROC_AUC= 0.9603559945236014\n",
      "Prevalence= 0.5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ann = MLPClassifier(learning_rate='constant',learning_rate_init=0.001, max_iter= 10000, activation='logistic',solver='sgd',hidden_layer_sizes=(35),random_state=1)\n",
    "ann.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_train_preds = ann.predict_proba(X_train)[:,1]\n",
    "y_test_preds = ann.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Training:')\n",
    "auc, accuracy, recall, precision, specificity = print_report(y_train,y_train_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_train,str_title=\"TRAINING \")\n",
    "\n",
    "print('\\n\\n\\nTest:')\n",
    "print_report(y_test,y_test_preds, thresh)\n",
    "printConfusionMatrix(accuracy,precision,recall,specificity,auc,y_test,str_title=\"TESTING \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:48:59.213331300Z",
     "start_time": "2024-02-22T19:46:48.754208400Z"
    }
   },
   "id": "660700bbfe470489"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:48:59.237176600Z",
     "start_time": "2024-02-22T19:48:59.217137300Z"
    }
   },
   "id": "461ca262a94ad02a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "42a42883e416c81f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
